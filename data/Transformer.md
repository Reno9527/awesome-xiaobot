# Transformer 最后一公里_小报童_Awesome_XiaoBOT

## Transformer 最后一公里介绍
> 作者董董灿，致力于帮助小伙伴快速入门AI算法，公众号《董董灿是个攻城狮》主理人。    
    
基于Transformer的大模型已经成为AI的顶流，该架构可以处理文本、语音和图像，基于此技术的AI应用也层出不穷。    
    
本专栏将围绕Transformer系统从零讲解背景与算法技术，帮你打通理解该架构的最后一公里。    
    
此外，也会有不少好玩的体验大模型应用的方法。    
    
目前内测，目前定价59.9元(可先占住名额)，内测结束后定价129，内测阶段购买也可免费终身阅读。    
    
购买后加我微信 ddcsggcs，祝你学有所成，收获满满。  
  


|名称|作者|读者数量|内容数量|更新时间|
|---|---|---|---|---|
|[Transformer 最后一公里](https://xiaobot.net/p/Transformer?refer=0b133df9-27dc-423b-8101-639049001c13)|董董灿|166人|64篇|2024-08-03|

## 最近更新
### Qwen2 的模型结构和细节

Qwen2（千问）是由阿里云开发的人工智能大模型，可用于智能对话，是一个典型的 Decoder-Only 结构。

该模型在很多任务上有着非常不错的表现，很多公司都会基于......

### GPT 和 BERT 的模型结构

上一节介绍了什么是 Decoder-Only 结构，并且提到，目前绝大多数的大模型采用的都是 Decoder-Only 结构。虽然如此，仍然一些模型会用到
Encoder 部分作为主要架构，典型......

### 什么是 Decoder-Only 结构

在《Attention is all you need》这篇论文中，作者给出了 Transformer 架构的完整图示。

一般来说，上图中左侧的部分被称为编码器，右侧的部分被称为解码......

### 后处理：预测得分的温度参数和 Softmax 计算

如果你调用过大模型的 API，或者创作过智能体的话，大概率你调整过一些用来控制模型生成效果的参数。

下图展示的是我在微调某个智能体时后台的参数设置界面，里面有一个多样性......

### 后处理：预测得分的 Top_p 采样

在上一节介绍了 Top_k 采样之后，接下来再看另一个非常常用的采样方法，叫做 Top_p 采样。

Top_p 采样中的 “p” 是 “probability”（概率......

### 后处理：预测得分的 Top_k 采样

在对大模型的输出 Logits 的后处理过程中，除了上一节提到的对分数进行重复惩罚之外，采样也是一个非常重要的步骤。

合理的采样算法和采样阈值的设置，可以使模型生成的内......

### 后处理：为什么要对预测得分进行惩罚？

从本节开始，会对大模型的后处理部分用到的一些经典的算法原理进行介绍。

所谓后处理，是指在大模型生成文本后，对生成的结果进行一系列的调整和优化，以确保输出的文本具有更好的......

### 如何实现 KVCache？

在上一节了解了 KVCache 提出的背景之后，本节来看一下如何实现 KVCache。

Q/K/V 都可以用矩阵来表示。在一个二维矩阵中，表示 K 和 V 的方式是一样......

### 大模型的推理过程：KVCache 的引入(Prefill 和 Decode)

在前面介绍完位置编码后，我们来从模型的视角看一个大模型中非常重要的技术，那就是 KVCache 缓存技术。

乍一看这个技术好像很深奥，又是 KV 又是缓存的，但是，如果......

### 位置编码：一文彻底搞懂旋转位置编码

上一节介绍了基于三角函数的位置编码，本节介绍旋转位置编码。

个人感觉旋转位置编码背后的思想非常的精妙，它的主要目的是利用一系列的算法，可以表达出句子内词与词之间的相对位......


<a href="https://github.com/Reno9527/awesome-xiaobot" style="color: white; text-decoration: none;">awesome-xiaobot</a>

返回 [首页](../README.md)
