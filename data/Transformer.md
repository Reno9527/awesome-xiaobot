|名称|作者|读者数量|内容数量|创建时间|更新时间|
---
|[Transformer 最后一公里](https://xiaobot.net/p/Transformer?refer=0b133df9-27dc-423b-8101-639049001c13)|@董董灿|128人|50篇|2024-04-18|2024-06-13|

# 最近更新
## 43、位置编码编码的是什么信息？前一节简单介绍了位置编码的作用：为文本施加单词的位置信息，解决 Transformer 中不存在位置信息的问题。如果你之前看到过关于位置编码的内容，你或许对位置编码会......
## 42、为什么需要位置编码？在了解了前面的一些基础算法的背景之后，本节来看一下与位置编码相关的介绍。在学习 NLP(自然语言处理），尤其是以 Transformer 为核心架构的大模型的过程中，......
## 41、什么是 Pre-Norm 和 Post-Norm？在阅读完 LayerNorm 和 残差连接之后，本节看一种将两者结合起来的结构：Pre-Norm 和 Post-Norm。从名字可以看出，Pre-Norm 和 Pos......
## 40、残差结构在 Transformer 中的作用前面介绍了 LayerNorm 运算的作用，主要就是完成对数据的归一化操作。在 Transformer 架构中，还存在一个比较经典的结构，那就是残差连接。残差连接最开......
## 39、为什么会有 LayerNorm 这类算法？接下来会花几节的篇幅来介绍为什么需要在 Transformer 架构中使用 LayerNorm 算法，以及这个算法的作用是啥？在很多关于大模型的面试中，都会问到这一算......
## 38、MaskSoftmax 如何屏蔽未来信息？上一节解释了 Softmax 这个算法的原理和使用场景。说白了，它的作用就是将输入数据转换为 [0-1] 之间的概率分布。Softmax 不仅仅会在模型的最后一层出现......
## 37、Softmax 是如何完成数值到概率转换的？接着一节的内容来继续看看，在模型最后一层全连接输出原始 Logits 之后，如何通过 Softmax 算法将其转换为概率的呢?如果你经常关注我的公众号文章，这部分你会......
## 36、模型的输出线性层：从隐藏特征到样本特征前面几节一直在介绍线性变换，相信看到这里你应该了解线性变换的主要作用了：它可以完成特征的升维或降维操作，关于这一点你可以查看 FFN 中的线性变换的内容来复习。
## 35、线性层和矩阵乘法的区别和联系在上一节介绍 FFN 中的线性层时，提到了一句话:对于线性层的实现，如果你对torch熟练的话，你可以用 torch.nn.functional.linear 函数来实现，也可以用......
## 34、FFN 中的线性变换接着上一节的非线性，本节继续看看 FFN 中的线性层。在 FFN 中，线性层用 Linear（线性）来表示。如果你对深度学习算法有较深的理解，你可以认为 FFN 实际......

